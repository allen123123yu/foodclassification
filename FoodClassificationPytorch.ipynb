{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fcea866-0db0-4bac-ac5d-28fa38284232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import ImageFolder\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26e012e-9666-40a2-ac3a-28bd6b37dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a682e87-8c4c-43ef-8b81-b13fb4fd2743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation pipeline for training\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   # Resize to 224x224\n",
    "    transforms.RandomHorizontalFlip(),                     # Horizontal flip\n",
    "    transforms.ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.5), saturation=(0.8, 1.3), hue=(-0.05, 0.05)),  # Color jitter\n",
    "    transforms.RandomRotation(15),                         # Random rotation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value='random'),  # Random erasing (Cutout)\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization\n",
    "])\n",
    "\n",
    "# Evaluation pipeline without augmentations\n",
    "eval_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6add4a73-4f65-4e84-8fac-b3e651013344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "train_dataset = ImageFolder(root='./train', transform=train_transforms)\n",
    "print(\"Class-to-Index Mapping:\", train_dataset.class_to_idx)\n",
    "print(\"Number of classes:\", len(train_dataset.classes))\n",
    "\n",
    "batch_size = 32\n",
    "dropout_rate = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d3f7d2-1b23-45ec-87cb-4f8e8b5dd6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for training and evaluation\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "eval_dataset = ImageFolder(root='./test', transform=eval_transforms)\n",
    "eval_loader = DataLoader(dataset=eval_dataset, batch_size=batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d923cdaf-d834-4bc2-abe5-c7c94f8cda11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class-to-Index Mapping: {'Baked Potato': 0, 'Crispy Chicken': 1, 'Donut': 2, 'Fries': 3, 'Hot Dog': 4, 'Sandwich': 5, 'Taco': 6, 'Taquito': 7, 'apple_pie': 8, 'burger': 9, 'butter_naan': 10, 'chai': 11, 'chapati': 12, 'cheesecake': 13, 'chicken_curry': 14, 'chole_bhature': 15, 'dal_makhani': 16, 'dhokla': 17, 'fried_rice': 18, 'ice_cream': 19, 'idli': 20, 'jalebi': 21, 'kaathi_rolls': 22, 'kadai_paneer': 23, 'kulfi': 24, 'masala_dosa': 25, 'momos': 26, 'omelette': 27, 'paani_puri': 28, 'pakode': 29, 'pav_bhaji': 30, 'pizza': 31, 'samosa': 32, 'sushi': 33}\n",
      "Number of classes: 34\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super(CombinedModel, self).__init__()\n",
    "\n",
    "        # Load pre-trained models (ResNet50 and EfficientNet B2)\n",
    "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        self.efficientnet = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        # Unfreeze all layers in both pre-trained models\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.efficientnet.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Remove final classification layers from both models\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n",
    "        self.efficientnet = nn.Sequential(*list(self.efficientnet.children())[:-1])\n",
    "\n",
    "        # Batch normalization layer for the combined feature map\n",
    "        self.bn = nn.BatchNorm1d(2048 + 1408)  # ResNet50 has 2048 features, EfficientNet B2 has 1408 features\n",
    "\n",
    "        # Fully connected layers for classification (34 classes)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2048 + 1408, 2048),  # Concatenate feature vectors\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(2048, 34)  # Output layer for 34 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features from both models\n",
    "        resnet_features = self.resnet(x).view(x.size(0), -1)  # Flatten the ResNet features\n",
    "        efficientnet_features = self.efficientnet(x).view(x.size(0), -1)  # Flatten the EfficientNet features\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat((resnet_features, efficientnet_features), dim=1)\n",
    "\n",
    "        # Apply batch normalization and ReLU activation after concatenation\n",
    "        combined_features = self.bn(combined_features)\n",
    "        combined_features = torch.relu(combined_features)\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15387cf2-0a7b-411e-b5cf-94dd10a9feda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = CombinedModel(dropout=dropout_rate).to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "lr = 0.001\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.resnet.parameters(), 'lr': lr * 0.1, 'weight_decay': 1e-3},\n",
    "    {'params': model.efficientnet.parameters(), 'lr': lr * 0.1, 'weight_decay': 1e-3},\n",
    "    {'params': model.fc.parameters(), 'lr': lr, 'weight_decay': 1e-3}\n",
    "])\n",
    "\n",
    "# Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17909b57-2200-471c-bc64-5a035531c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, eval_loader, criterion, optimizer, num_epochs):\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with tqdm(total=len(train_loader), desc=f\"Train Epoch [{epoch+1}/{num_epochs}]\") as pbar:\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                total += labels.size(0)\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "        # Calculate metrics\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Train F1: {epoch_f1:.4f}\")\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        eval_loss, eval_acc, eval_f1 = evaluate_model(model, eval_loader, criterion, epoch, num_epochs)\n",
    "        scheduler.step(eval_loss)\n",
    "\n",
    "        if eval_f1 > best_f1:\n",
    "            best_f1 = eval_f1\n",
    "            print(f\"Best model saved with Eval F1: {best_f1:.4f}\")\n",
    "            torch.save(model.state_dict(), 'best_model_34_classes.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef6c7748-ea7d-4fae-a450-a2d0928924f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, eval_loader, criterion, epoch, num_epochs):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with tqdm(total=len(eval_loader), desc=f\"Eval Epoch [{epoch+1}/{num_epochs}]\") as pbar:\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in eval_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                pbar.update(1)\n",
    "\n",
    "    eval_loss = running_loss / total\n",
    "    eval_acc = correct / total\n",
    "    eval_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(f\"Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_acc:.4f}, Eval F1: {eval_f1:.4f}\")\n",
    "    return eval_loss, eval_acc, eval_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0aed010-710a-43ad-8e61-ba8c9fc38ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch [1/10]: 100%|██████████| 597/597 [01:52<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 1.1947, Train F1: 0.6295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval Epoch [1/10]: 100%|██████████| 150/150 [00:29<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.6433, Eval Accuracy: 0.8142, Eval F1: 0.8148\n",
      "Best model saved with Eval F1: 0.8148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch [2/10]: 100%|██████████| 597/597 [02:06<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Train Loss: 0.7357, Train F1: 0.7793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval Epoch [2/10]: 100%|██████████| 150/150 [00:28<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.5589, Eval Accuracy: 0.8492, Eval F1: 0.8566\n",
      "Best model saved with Eval F1: 0.8566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch [3/10]: 100%|██████████| 597/597 [01:51<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Train Loss: 0.6264, Train F1: 0.8166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval Epoch [3/10]: 100%|██████████| 150/150 [00:29<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.4575, Eval Accuracy: 0.8691, Eval F1: 0.8799\n",
      "Best model saved with Eval F1: 0.8799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch [4/10]: 100%|██████████| 597/597 [01:53<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Train Loss: 0.6031, Train F1: 0.8307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval Epoch [4/10]: 100%|██████████| 150/150 [00:41<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.4534, Eval Accuracy: 0.8821, Eval F1: 0.8849\n",
      "Best model saved with Eval F1: 0.8849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch [5/10]: 100%|██████████| 597/597 [03:55<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Train Loss: 0.5328, Train F1: 0.8489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval Epoch [5/10]: 100%|██████████| 150/150 [00:39<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.4129, Eval Accuracy: 0.8921, Eval F1: 0.8980\n",
      "Best model saved with Eval F1: 0.8980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch [6/10]: 100%|██████████| 597/597 [03:59<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Train Loss: 0.3376, Train F1: 0.9022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval Epoch [6/10]: 100%|██████████| 150/150 [00:40<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.3224, Eval Accuracy: 0.9139, Eval F1: 0.9217\n",
      "Best model saved with Eval F1: 0.9217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch [7/10]: 100%|██████████| 597/597 [04:00<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Train Loss: 0.2857, Train F1: 0.9166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval Epoch [7/10]: 100%|██████████| 150/150 [00:39<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.3208, Eval Accuracy: 0.9158, Eval F1: 0.9226\n",
      "Best model saved with Eval F1: 0.9226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch [8/10]: 100%|██████████| 597/597 [03:40<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Train Loss: 0.2716, Train F1: 0.9198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval Epoch [8/10]: 100%|██████████| 150/150 [00:38<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.3645, Eval Accuracy: 0.9141, Eval F1: 0.9256\n",
      "Best model saved with Eval F1: 0.9256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch [9/10]: 100%|██████████| 597/597 [04:11<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Train Loss: 0.2742, Train F1: 0.9189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval Epoch [9/10]: 100%|██████████| 150/150 [00:42<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.3541, Eval Accuracy: 0.9102, Eval F1: 0.9196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch [10/10]: 100%|██████████| 597/597 [03:46<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Train Loss: 0.1997, Train F1: 0.9417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval Epoch [10/10]: 100%|██████████| 150/150 [00:36<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.3009, Eval Accuracy: 0.9250, Eval F1: 0.9361\n",
      "Best model saved with Eval F1: 0.9361\n"
     ]
    }
   ],
   "source": [
    "# Run the training and evaluation\n",
    "train_model(model, train_loader, eval_loader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32080c6-70ad-4da1-af60-3ef279bd0331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d6d3c1-b348-4832-9969-75cf952e3552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
